{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cddcbd7",
   "metadata": {},
   "source": [
    "## Goal\n",
    "To see if a generalized Dino model trained on 1-dimensional channels can perform as well as the specialized model trained on 4-dimensional images. The HPA FOV 4-channel images are the only dataset used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be56b8a",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Images were downloaded using the `custom_scripts/HPA_IMG_download.py` script, and then were cleaned with scripts `img_corrupted_check.py` and `img_corrupted_check.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9d026",
   "metadata": {},
   "source": [
    "### Package Environment\n",
    "I’ve made a Conda environment to freeze the libraries that I’m using: `conda_cuda12.yaml`\n",
    "\n",
    "\n",
    "However, to do data visualization and analysis, a much larger set of libraries was needed in addition, for which I made an extended environment:\n",
    "`conda_cuda12_notebook_analysis.yaml`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b86ae8",
   "metadata": {},
   "source": [
    "### Compute Environment\n",
    "This was done on Google Cloud using their V100 GPU instances, and then the data was transferred to their cpu-only instances under free acount nickmarveaux@gmail.com. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a93b02",
   "metadata": {},
   "source": [
    "## Experimental Design\n",
    "Create a 4d model and a 1d model of Dino, trained on the same ~75,000 images that have been resized to 512x512, and all other parameters being kept the same (trained for the same number of epochs, 50, and the same transformer Vit_Tiny). Then, generate feature embeddings of those ~75,000 images using each Dino model independently, and train a classifier on them independently, with again all parameteres being equal. One notable difference is that the input to the classifier trained on 4-D images is vectors of size 192, whereas the input to the second classifier is a stack of 4 1-Dimensional embeddings of that size, so is a vector of size 768. Finally, the 2 classifiers are evaluated on the same test data from Kaggle's HPA competition using the same process: embeddings are generated by the two dino models, stacked in the case of the 1-D Dino model, then passed to the classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7dc2b7",
   "metadata": {},
   "source": [
    "### Configs\n",
    "\n",
    "All configs for this project were placed in directory exploratory_configs\n",
    "\n",
    "The 4d Dino model was generated with yaml —  and is located at — on instance-11\n",
    "\n",
    "The 1d Dino model was generated with yaml —  and is located at — on instance-11\n",
    "\n",
    "The 1d classifier was generated with yaml and is located at on instance-11\n",
    "\n",
    "The 4d classifier was generated with yaml and is located at on instance-11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eeea78",
   "metadata": {},
   "source": [
    "### Evalutation\n",
    "\n",
    "The program prepare_kaggle_submission.py was run by \n",
    "\n",
    "It was modified to output the highest probability class even when there is uncertainty, in this commit: https://github.com/nickdeveaux/Dino4Cells_analysis/commit/e17db1a9815c95de2a4f4d79fb2d83c44df22f9e\n",
    "\n",
    "The submission.csv received these scores:\n",
    "\n",
    "#### 1-D\n",
    "Score: 0.21374\n",
    "Private score: 0.189\n",
    "Submitted: January 18 2024\n",
    "\n",
    "#### 4-D\n",
    "Score: 0.21157\n",
    "Private score: 0.18859\n",
    "Submitted: January 7 20224\n",
    "\n",
    "Submitted here on Kaggle https://www.kaggle.com/c/human-protein-atlas-image-classification/data under account nrdeveaux@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798c4d4",
   "metadata": {},
   "source": [
    "### Learnings:\n",
    "* Was originally seeing close to Zero nvidia-smi usage, due to the images being too large, and so the majority of time was spent on the cpu resiziing the images down to 512x512. \n",
    "* The data cleanliness was also an issue, due to using a script to download the data, had to specifically clean any corrupted files, and had to do that again when splitting it into 1d pngs, the list of vallid 1d pngs for training was not 1:1 with the training labels in the training csv leading to useless classifiers that would do well on the training set only to output nonsense due to being trained on jumbled labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9082b0",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d85b3",
   "metadata": {},
   "source": [
    "### Useful commands\n",
    "* `watch -n 0.5 nvidia-smi` for viewing usage of gpu\n",
    "* `sudo su nick` had to be run on the nickmarveaux account to get to the proper account. \n",
    "* branch `origin/ndv_run_end_to_end_without_pretrained_features` was used as of December to make modifications to the run_get_features.py scripts, since I had low confidence of fully understanding the impact of those code changes.\n",
    "* `gcloud auth login` needed to switch between google cloud accounts\n",
    "* `gcloud compute ssh --zone \"us-east4-c\" \"instance-11\" --project \"focal-slice-407815\"` focal-slice-40781 is the ID of the GCP project, and this is the ssh command to get onto the instance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
